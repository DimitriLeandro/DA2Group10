{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DimitriLeandro/DA2Group10/blob/main/final_project_part_i.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "43a020db",
      "metadata": {
        "id": "43a020db"
      },
      "source": [
        "# Data Analytics II - Final Project Part I"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2e0d92f7",
      "metadata": {
        "id": "2e0d92f7"
      },
      "source": [
        "## Methodology:\n",
        "\n",
        "- Feature scaling\n",
        "\n",
        "- Feature importance using Gini Index\n",
        "\n",
        "- Deletion of unnecessary features\n",
        "\n",
        "- Correlation between features\n",
        "\n",
        "For each classifier:\n",
        "\n",
        "    - SFS analysis using standard hyperparameters\n",
        "    \n",
        "    - Gridsearch using the selected features\n",
        "\n",
        "    - Analysis of classification metrics\n",
        "\n",
        "- Fitting of the best model with all the training data\n",
        "\n",
        "- Predictions to the test dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5fd7cc4f",
      "metadata": {
        "id": "5fd7cc4f"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "9388edd0",
      "metadata": {
        "id": "9388edd0"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import plotly.graph_objects as go\n",
        "import plotly.express as px\n",
        "import seaborn as sb\n",
        "import matplotlib.pyplot as plt\n",
        "import warnings\n",
        "import json\n",
        "import xgboost as xgb\n",
        "from time import time\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
        "from sklearn.model_selection import GridSearchCV, KFold, train_test_split\n",
        "from sklearn.feature_selection import SequentialFeatureSelector \n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "#!pip install -U kaleido"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "wuZo0unlIo5N",
      "metadata": {
        "id": "wuZo0unlIo5N"
      },
      "source": [
        "## Downloading and unziping datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "aZoa22InInfy",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aZoa22InInfy",
        "outputId": "26a3686c-575a-485b-8881-521a640f8326"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-06-27 17:10:22--  https://uni-muenster.sciebo.de/s/bmzyEnwSscZ0tam/download?path=%2F\n",
            "Resolving uni-muenster.sciebo.de (uni-muenster.sciebo.de)... 128.176.4.4\n",
            "Connecting to uni-muenster.sciebo.de (uni-muenster.sciebo.de)|128.176.4.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [application/zip]\n",
            "Saving to: ‘download?path=%2F.1’\n",
            "\n",
            "download?path=%2F.1     [          <=>       ]  36.05M  17.0MB/s    in 2.1s    \n",
            "\n",
            "2022-06-27 17:10:25 (17.0 MB/s) - ‘download?path=%2F.1’ saved [37799682]\n",
            "\n",
            "replace task_1/test_set.csv? [y]es, [n]o, [A]ll, [N]one, [r]ename: N\n"
          ]
        }
      ],
      "source": [
        "!wget https://uni-muenster.sciebo.de/s/bmzyEnwSscZ0tam/download?path=%2F&files=train_set.csv\n",
        "!unzip -qq /content/download?path=%2F"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "BBvRsrAZK9Ej",
      "metadata": {
        "id": "BBvRsrAZK9Ej"
      },
      "source": [
        "## Mounting drive to save results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "tnNrpj30LDID",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tnNrpj30LDID",
        "outputId": "d6a9457b-0fc6-4527-bb87-707d4bfefcc0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c8faec9d",
      "metadata": {
        "id": "c8faec9d"
      },
      "source": [
        "## Training dataset preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b5af4a30",
      "metadata": {
        "id": "b5af4a30"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv('/content/task_1/train_set.csv') # /home/dimi/Downloads/task_1_datasets/train_set.csv\n",
        "y  = df['y']\n",
        "X  = pd.DataFrame(\n",
        "    data    = StandardScaler().fit_transform(df.drop('y', axis=1)),\n",
        "    columns = df.columns.drop('y')\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "79134040",
      "metadata": {
        "id": "79134040"
      },
      "outputs": [],
      "source": [
        "df_label_counts = pd.DataFrame(\n",
        "    data    = np.transpose(np.unique(y, return_counts=True)),\n",
        "    columns = ['label', 'count'] \n",
        ").sort_values(\n",
        "    by        = 'count',\n",
        "    ascending = False\n",
        ").reset_index(\n",
        "    drop = True\n",
        ")\n",
        "\n",
        "df_label_counts['pct'] = 100*df_label_counts['count']/y.size\n",
        "\n",
        "df_label_counts"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7a7e1440",
      "metadata": {
        "id": "7a7e1440"
      },
      "source": [
        "## Feature Importance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "faf86fed",
      "metadata": {
        "id": "faf86fed"
      },
      "outputs": [],
      "source": [
        "# rf = RandomForestClassifier(\n",
        "#     n_estimators = 1000,\n",
        "#     n_jobs       = -1,\n",
        "#     max_samples  = 0.66\n",
        "# )\n",
        "# rf.fit(X, y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "XGhB-oYwPNtE",
      "metadata": {
        "id": "XGhB-oYwPNtE"
      },
      "outputs": [],
      "source": [
        "# df_feature_importances = pd.DataFrame(\n",
        "#     data    = zip(df.columns.drop('y'), rf.feature_importances_),\n",
        "#     columns = ['feature', 'importance'] \n",
        "# ).sort_values(\n",
        "#     by        = 'importance',\n",
        "#     ascending = False,\n",
        "# ).reset_index(\n",
        "#     drop = True\n",
        "# )\n",
        "\n",
        "# df_feature_importances.to_csv(\n",
        "#     'results/task_1/feature_importance_final_project_part_i.csv', #'/content/drive/MyDrive/Colab Notebooks/feature_importance_final_project_part_i.csv', \n",
        "#     index = False\n",
        "# )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "be6f239f",
      "metadata": {
        "id": "be6f239f"
      },
      "outputs": [],
      "source": [
        "max_features = 30\n",
        "df_feature_importances = pd.read_csv('results/task_1/feature_importance_final_project_part_i.csv')\n",
        "fig = px.bar(\n",
        "    data_frame = df_feature_importances[:max_features],\n",
        "    x          = 'feature',\n",
        "    y          = 'importance'\n",
        ")\n",
        "fig.update_xaxes(tickangle=90)\n",
        "fig.write_image('results/task_1/feature_importance_final_project_part_i.pdf')\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e4952b2a",
      "metadata": {
        "id": "e4952b2a"
      },
      "source": [
        "## Correlation between features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "16b209ee",
      "metadata": {
        "id": "16b209ee"
      },
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(figsize=(18,14))\n",
        "sb.heatmap(X[df_feature_importances.loc[:8, 'feature']].corr(), cmap=\"Blues\", annot=True, linewidths=0.1)\n",
        "fig.savefig('results/task_1/feature_correlations_final_project_part_i.pdf')\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e2172cf3",
      "metadata": {
        "id": "e2172cf3"
      },
      "source": [
        "Features are not correlated: no need to remove correlated features to prevent overfitting!"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ff3a8d56",
      "metadata": {
        "id": "ff3a8d56"
      },
      "source": [
        "## SFS to each classifier\n",
        "\n",
        "OBS: no need for SFS when using Random Forests."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "777e32de",
      "metadata": {
        "id": "777e32de"
      },
      "outputs": [],
      "source": [
        "n_jobs               = -1\n",
        "cv                   = 10\n",
        "verbose              = 3\n",
        "n_features_to_select = 100\n",
        "plot_step            = 10\n",
        "scoring              = 'accuracy'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2ad35cff",
      "metadata": {
        "id": "2ad35cff"
      },
      "outputs": [],
      "source": [
        "def run_sfs(estimator, X, y, n_features_to_select, cv, scoring, n_jobs, sorted_features=None, plot_step=1):\n",
        "    \n",
        "    if sorted_features is None:\n",
        "        sfs = SequentialFeatureSelector(\n",
        "            estimator            = estimator,\n",
        "            direction            = 'forward',\n",
        "            n_features_to_select = n_features_to_select,\n",
        "            n_jobs               = n_jobs,\n",
        "            cv                   = cv,\n",
        "            scoring              = scoring,\n",
        "        )\n",
        "        sfs.fit(X, y)\n",
        "        sorted_features = sfs.get_feature_names_out()\n",
        "    \n",
        "    kf               = KFold(n_splits=cv, shuffle=True)\n",
        "    accs             = []\n",
        "    stds             = []\n",
        "    n_features_range = np.arange(plot_step, n_features_to_select+1, plot_step)\n",
        "\n",
        "    for n_features in n_features_range:\n",
        "        print(n_features)\n",
        "        features   = sorted_features[:n_features]\n",
        "        inner_accs = []\n",
        "\n",
        "        for train_index, test_index in kf.split(X):\n",
        "            X_train, X_test = X.loc[train_index][features], X.loc[test_index][features]\n",
        "            y_train, y_test = y.loc[train_index], y.loc[test_index]\n",
        "            clf = estimator\n",
        "            clf.fit(X_train, y_train)\n",
        "            y_pred = clf.predict(X_test)\n",
        "            inner_accs.append(accuracy_score(y_test, y_pred))\n",
        "\n",
        "        accs.append(np.mean(inner_accs))\n",
        "        stds.append(np.std(inner_accs))\n",
        "\n",
        "    fig = px.line(\n",
        "        x       = n_features_range,\n",
        "        y       = accs,\n",
        "        error_y = stds,\n",
        "        title   = 'Sequential Feature Selector: ' + estimator.__class__.__name__,\n",
        "        labels  = dict(\n",
        "            x = 'Number of features', \n",
        "            y = 'Accuracy'\n",
        "        )\n",
        "    )\n",
        "    fig.update_xaxes(tickvals=n_features_range)\n",
        "    fig.write_image('results/task_1/sfs_{}_final_project_part_i.pdf'.format(estimator.__class__.__name__))\n",
        "    fig.show()\n",
        "    \n",
        "    np.savetxt(\n",
        "        fname = 'results/task_1/sfs_{}_final_project_part_i.csv'.format(estimator.__class__.__name__), \n",
        "        X     = sorted_features, \n",
        "        fmt   = '%s'\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3ed38e43",
      "metadata": {
        "id": "3ed38e43"
      },
      "source": [
        "### LDA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b059a38e",
      "metadata": {
        "id": "b059a38e"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "run_sfs(\n",
        "    LinearDiscriminantAnalysis(),\n",
        "    X[df_feature_importances.loc[:100, 'feature']],\n",
        "    y,\n",
        "    n_features_to_select,\n",
        "    cv,\n",
        "    scoring,\n",
        "    n_jobs,\n",
        "    df_feature_importances['feature'],\n",
        "    plot_step\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "47eed40c",
      "metadata": {
        "id": "47eed40c"
      },
      "source": [
        "### KNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "12a070ea",
      "metadata": {
        "id": "12a070ea"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "run_sfs(\n",
        "    KNeighborsClassifier(n_jobs=n_jobs),\n",
        "    X[df_feature_importances.loc[:100, 'feature']],\n",
        "    y,\n",
        "    n_features_to_select,\n",
        "    cv,\n",
        "    scoring,\n",
        "    n_jobs,\n",
        "    df_feature_importances['feature'],\n",
        "    plot_step\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "47debd3c",
      "metadata": {
        "id": "47debd3c"
      },
      "source": [
        "### SVM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "80f16c9d",
      "metadata": {
        "id": "80f16c9d"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "run_sfs(\n",
        "    SVC(),\n",
        "    X[df_feature_importances.loc[:100, 'feature']],\n",
        "    y,\n",
        "    n_features_to_select,\n",
        "    cv,\n",
        "    scoring,\n",
        "    n_jobs,\n",
        "    df_feature_importances['feature'],\n",
        "    plot_step\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2959597f",
      "metadata": {
        "id": "2959597f"
      },
      "source": [
        "### LR"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3561d5d4",
      "metadata": {
        "id": "3561d5d4"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "run_sfs(\n",
        "    LogisticRegression(n_jobs=n_jobs),\n",
        "    X[df_feature_importances.loc[:100, 'feature']],\n",
        "    y,\n",
        "    n_features_to_select,\n",
        "    cv,\n",
        "    scoring,\n",
        "    n_jobs,\n",
        "    df_feature_importances['feature'],\n",
        "    plot_step\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "be1736bf",
      "metadata": {
        "id": "be1736bf"
      },
      "source": [
        "## Gridsearch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3a429979",
      "metadata": {
        "id": "3a429979"
      },
      "outputs": [],
      "source": [
        "def run_gridsearch(estimator, X, y, param_grid, cv, scoring, n_jobs, verbose):\n",
        "    gs = GridSearchCV(\n",
        "        estimator  = estimator,\n",
        "        n_jobs     = n_jobs,\n",
        "        cv         = cv,\n",
        "        verbose    = verbose,\n",
        "        scoring    = scoring,\n",
        "        param_grid = param_grid\n",
        "    )\n",
        "    gs.fit(X, y)\n",
        "    results = {\n",
        "        'Estimator': str(estimator.__class__.__name__),\n",
        "        'Number of features': X.shape[1],\n",
        "        'Best result': '{:.3f} +- {:.3f}'.format(\n",
        "            float(gs.cv_results_['mean_test_score'][gs.best_index_]),\n",
        "            float(gs.cv_results_['std_test_score'][gs.best_index_]),\n",
        "        ),\n",
        "        'Best hyperparameters': gs.best_params_\n",
        "    }\n",
        "    with open(\n",
        "        file = 'results/task_1/gridsearch_{}_final_project_part_i.json'.format(estimator.__class__.__name__),\n",
        "        mode = 'w'\n",
        "    ) as file:\n",
        "        json.dump(results, file, indent=4)\n",
        "    print(results)\n",
        "    return gs"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ecf44471",
      "metadata": {
        "id": "ecf44471"
      },
      "source": [
        "### Random Forest"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5cd30bd4",
      "metadata": {
        "id": "5cd30bd4"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "\n",
        "param_grid = dict(\n",
        "    n_estimators      = [1000],\n",
        "    criterion         = ['gini'],\n",
        "    max_depth         = [10, 11, 12],\n",
        "    class_weight      = [None],\n",
        "    max_samples       = [0.66, 0.75, 1]\n",
        ")\n",
        "\n",
        "run_gridsearch(\n",
        "    RandomForestClassifier(), \n",
        "    X[df_feature_importances.loc[:100, 'feature']], \n",
        "    y, \n",
        "    param_grid, \n",
        "    cv, \n",
        "    scoring, \n",
        "    n_jobs, \n",
        "    verbose\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c1e4a6e8",
      "metadata": {
        "id": "c1e4a6e8"
      },
      "source": [
        "### KNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b92e4bc5",
      "metadata": {
        "id": "b92e4bc5"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "\n",
        "param_grid = dict(\n",
        "    n_neighbors = [5, 10, 20, 30], #large K prevents overfitting!\n",
        ")\n",
        "\n",
        "run_gridsearch(\n",
        "    KNeighborsClassifier(), \n",
        "    X[df_feature_importances.loc[:60, 'feature']], \n",
        "    y, \n",
        "    param_grid, \n",
        "    cv, \n",
        "    scoring, \n",
        "    1, # use just one kernel with KNN, otherwise, memory fuuuuuull \n",
        "    verbose\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "09dde873",
      "metadata": {
        "id": "09dde873"
      },
      "source": [
        "### SVM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7a492583",
      "metadata": {
        "id": "7a492583"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "\n",
        "param_grid = dict(\n",
        "    kernel = ['linear', 'poly', 'rbf'],\n",
        "    degree = [2, 3]\n",
        ")\n",
        "\n",
        "run_gridsearch(\n",
        "    SVC(), \n",
        "    X[df_feature_importances.loc[:50, 'feature']], \n",
        "    y, \n",
        "    param_grid, \n",
        "    cv, \n",
        "    scoring, \n",
        "    n_jobs, \n",
        "    verbose\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a525b728",
      "metadata": {
        "id": "a525b728"
      },
      "source": [
        "### LR"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9234549e",
      "metadata": {
        "id": "9234549e"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "\n",
        "param_grid = dict(\n",
        "    solver   = ['saga'],\n",
        "    penalty  = ['elasticnet'],\n",
        "    C        = [0.5, 0.75, 1, 1.25, 1.5],\n",
        "    l1_ratio = [0.01, 0.25, 0.5, 0.75, 0.99]\n",
        ")\n",
        "\n",
        "run_gridsearch(\n",
        "    LogisticRegression(), \n",
        "    X[df_feature_importances.loc[:80, 'feature']], \n",
        "    y, \n",
        "    param_grid, \n",
        "    cv, \n",
        "    scoring, \n",
        "    n_jobs, \n",
        "    verbose\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5a1dc78c",
      "metadata": {
        "id": "5a1dc78c"
      },
      "source": [
        "### LDA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9da2b89b",
      "metadata": {
        "id": "9da2b89b"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "\n",
        "param_grid = dict()\n",
        "\n",
        "run_gridsearch(\n",
        "    LinearDiscriminantAnalysis(), \n",
        "    X[df_feature_importances.loc[:90, 'feature']], \n",
        "    y, \n",
        "    param_grid, \n",
        "    cv, \n",
        "    scoring, \n",
        "    n_jobs, \n",
        "    verbose\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## XG Boost"
      ],
      "metadata": {
        "id": "PVKC7d7FnXo7"
      },
      "id": "PVKC7d7FnXo7"
    },
    {
      "cell_type": "code",
      "source": [
        "# encoding y\n",
        "encoder   = LabelEncoder()\n",
        "y_encoder = encoder.fit_transform(y)\n",
        "\n",
        "# random split to start with (true cross validation later)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y_encoder, test_size=0.2, random_state=0)\n",
        "\n",
        "# xgb data matrices\n",
        "D_train = xgb.DMatrix(X_train, label=y_train)\n",
        "D_test  = xgb.DMatrix(X_test, label=y_test)"
      ],
      "metadata": {
        "id": "NPywmOvfnbDJ"
      },
      "id": "NPywmOvfnbDJ",
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### XGB gridsearch (without sklearn API)\n",
        "\n",
        "The reason not to use sklearn API is to speed up the process by using GPU support"
      ],
      "metadata": {
        "id": "f-63fZ1dRkaq"
      },
      "id": "f-63fZ1dRkaq"
    },
    {
      "cell_type": "code",
      "source": [
        "initial_params = {\n",
        "    'num_parallel_tree': 10, # used for boosting random forest\n",
        "    'max_depth':         5,  # maximum tree depth for base learners\n",
        "    'eta':               0.25, # boosting learning rate \n",
        "    'subsample':         0.75, # subsample ratio of the training instance\n",
        "    'colsample_bytree':  0.75, # subsample ratio of columns when constructing each tree\n",
        "    'colsample_bylevel': 1, # subsample ratio of columns when constructing each tree\n",
        "    'colsample_bynode':  1, # subsample ratio of columns when constructing each tree\n",
        "    'gamma':             1,  # minimum loss reduction required to make a further partition on a leaf\n",
        "    'alpha':             0,  # L1 regularization term on weights \n",
        "    'lambda':            1,  # L2 regularization term on weights\n",
        "    'sampling_method':   'gradient_based', # select samples based in their errors (not uniform)\n",
        "    'tree_method':       'gpu_hist',     # make it fast\n",
        "    'predictor':         'gpu_predictor' # make it fast\n",
        "}\n",
        "\n",
        "grid_params = {\n",
        "    'max_depth':         [3, 5, 8, 10, 12, 15], # maximum tree depth for base learners\n",
        "    'gamma':             [0, 1, 5, 10],   # minimum loss reduction required to make a further partition on a leaf\n",
        "    'alpha':             [0, 0.5, 1, 1.5, 2], # L1 regularization term on weights \n",
        "    'lambda':            [0, 0.5, 1, 1.5, 2], # L2 regularization term on weights\n",
        "    'subsample':         [0.66, 0.75, 1], # subsample ratio of the training instance\n",
        "    'colsample_bytree':  [0.66, 0.75, 1], # subsample ratio of columns when constructing each tree\n",
        "    'colsample_bylevel': [0.66, 0.75, 1], # subsample ratio of columns when constructing each tree\n",
        "    'colsample_bynode':  [0.66, 0.75, 1], # subsample ratio of columns when constructing each tree\n",
        "    'eta':               [0.1, 0.2, 0.25, 0.33],  # boosting learning rate \n",
        "}"
      ],
      "metadata": {
        "id": "NQKEN-ppndNF"
      },
      "id": "NQKEN-ppndNF",
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "best_params = initial_params.copy()\n",
        "for param, values in grid_params.items():\n",
        "  best_score = 9e9\n",
        "  print('\\nbest_params:', best_params)\n",
        "  for value in values:\n",
        "    print('testing {}={}'.format(param, value))\n",
        "    xgb_params = best_params.copy()\n",
        "    xgb_params.update({param: value})\n",
        "    xgb_model = xgb.train(\n",
        "        params                = xgb_params,\n",
        "        dtrain                = D_train,\n",
        "        evals                 = [(D_test,'eval')],\n",
        "        num_boost_round       = 150,\n",
        "        early_stopping_rounds = 5,\n",
        "        verbose_eval          = 0,\n",
        "    )\n",
        "    if xgb_model.best_score < best_score:\n",
        "      best_score = xgb_model.best_score\n",
        "      best_params.update({param: value})\n",
        "      print('new best value found: {}={} ({})'.format(param, value, best_score))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qle3C5TdENax",
        "outputId": "0c20b88f-557b-4fc7-8f5b-ed38f98c8331"
      },
      "id": "qle3C5TdENax",
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "best_params: {'num_parallel_tree': 10, 'max_depth': 5, 'eta': 0.25, 'subsample': 0.75, 'colsample_bytree': 0.75, 'colsample_bylevel': 1, 'colsample_bynode': 1, 'gamma': 1, 'alpha': 0, 'lambda': 1, 'sampling_method': 'gradient_based', 'tree_method': 'gpu_hist', 'predictor': 'gpu_predictor'}\n",
            "testing gamma=0\n",
            "new best value found: gamma=0 (1.210185)\n",
            "testing gamma=1\n",
            "testing gamma=5\n",
            "testing gamma=10\n",
            "\n",
            "best_params: {'num_parallel_tree': 10, 'max_depth': 5, 'eta': 0.25, 'subsample': 0.75, 'colsample_bytree': 0.75, 'colsample_bylevel': 1, 'colsample_bynode': 1, 'gamma': 0, 'alpha': 0, 'lambda': 1, 'sampling_method': 'gradient_based', 'tree_method': 'gpu_hist', 'predictor': 'gpu_predictor'}\n",
            "testing alpha=0\n",
            "new best value found: alpha=0 (1.210185)\n",
            "testing alpha=0.5\n",
            "testing alpha=1\n",
            "testing alpha=1.5\n",
            "testing alpha=2\n",
            "\n",
            "best_params: {'num_parallel_tree': 10, 'max_depth': 5, 'eta': 0.25, 'subsample': 0.75, 'colsample_bytree': 0.75, 'colsample_bylevel': 1, 'colsample_bynode': 1, 'gamma': 0, 'alpha': 0, 'lambda': 1, 'sampling_method': 'gradient_based', 'tree_method': 'gpu_hist', 'predictor': 'gpu_predictor'}\n",
            "testing lambda=0\n",
            "new best value found: lambda=0 (1.208107)\n",
            "testing lambda=0.5\n",
            "testing lambda=1\n",
            "testing lambda=1.5\n",
            "testing lambda=2\n",
            "\n",
            "best_params: {'num_parallel_tree': 10, 'max_depth': 5, 'eta': 0.25, 'subsample': 0.75, 'colsample_bytree': 0.75, 'colsample_bylevel': 1, 'colsample_bynode': 1, 'gamma': 0, 'alpha': 0, 'lambda': 0, 'sampling_method': 'gradient_based', 'tree_method': 'gpu_hist', 'predictor': 'gpu_predictor'}\n",
            "testing subsample=0.66\n",
            "new best value found: subsample=0.66 (1.225825)\n",
            "testing subsample=0.75\n",
            "new best value found: subsample=0.75 (1.208107)\n",
            "testing subsample=1\n",
            "\n",
            "best_params: {'num_parallel_tree': 10, 'max_depth': 5, 'eta': 0.25, 'subsample': 0.75, 'colsample_bytree': 0.75, 'colsample_bylevel': 1, 'colsample_bynode': 1, 'gamma': 0, 'alpha': 0, 'lambda': 0, 'sampling_method': 'gradient_based', 'tree_method': 'gpu_hist', 'predictor': 'gpu_predictor'}\n",
            "testing colsample_bytree=0.66\n",
            "new best value found: colsample_bytree=0.66 (1.213759)\n",
            "testing colsample_bytree=0.75\n",
            "new best value found: colsample_bytree=0.75 (1.208107)\n",
            "testing colsample_bytree=1\n",
            "\n",
            "best_params: {'num_parallel_tree': 10, 'max_depth': 5, 'eta': 0.25, 'subsample': 0.75, 'colsample_bytree': 0.75, 'colsample_bylevel': 1, 'colsample_bynode': 1, 'gamma': 0, 'alpha': 0, 'lambda': 0, 'sampling_method': 'gradient_based', 'tree_method': 'gpu_hist', 'predictor': 'gpu_predictor'}\n",
            "testing colsample_bylevel=0.66\n",
            "new best value found: colsample_bylevel=0.66 (1.217138)\n",
            "testing colsample_bylevel=0.75\n",
            "new best value found: colsample_bylevel=0.75 (1.212776)\n",
            "testing colsample_bylevel=1\n",
            "new best value found: colsample_bylevel=1 (1.208107)\n",
            "\n",
            "best_params: {'num_parallel_tree': 10, 'max_depth': 5, 'eta': 0.25, 'subsample': 0.75, 'colsample_bytree': 0.75, 'colsample_bylevel': 1, 'colsample_bynode': 1, 'gamma': 0, 'alpha': 0, 'lambda': 0, 'sampling_method': 'gradient_based', 'tree_method': 'gpu_hist', 'predictor': 'gpu_predictor'}\n",
            "testing colsample_bynode=0.66\n",
            "new best value found: colsample_bynode=0.66 (1.22089)\n",
            "testing colsample_bynode=0.75\n",
            "new best value found: colsample_bynode=0.75 (1.212503)\n",
            "testing colsample_bynode=1\n",
            "new best value found: colsample_bynode=1 (1.208107)\n",
            "\n",
            "best_params: {'num_parallel_tree': 10, 'max_depth': 5, 'eta': 0.25, 'subsample': 0.75, 'colsample_bytree': 0.75, 'colsample_bylevel': 1, 'colsample_bynode': 1, 'gamma': 0, 'alpha': 0, 'lambda': 0, 'sampling_method': 'gradient_based', 'tree_method': 'gpu_hist', 'predictor': 'gpu_predictor'}\n",
            "testing eta=0.1\n",
            "new best value found: eta=0.1 (1.246057)\n",
            "testing eta=0.2\n",
            "new best value found: eta=0.2 (1.222029)\n",
            "testing eta=0.3\n",
            "new best value found: eta=0.3 (1.218986)\n",
            "testing eta=0.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Saving results and using CV to measure the accuracy"
      ],
      "metadata": {
        "id": "vdbhN-5oR9mM"
      },
      "id": "vdbhN-5oR9mM"
    },
    {
      "cell_type": "code",
      "source": [
        "best_params.update({\n",
        "    'num_parallel_tree': 25,              # incresing number of trees\n",
        "    'num_class':         20,              # need this parameter if we want to use softmax\n",
        "    'objective':         'multi:softmax', # for multiclass classification (use simple rmse to make it go faster)\n",
        "})\n",
        "\n",
        "best_params"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OHUDfbHTNeL5",
        "outputId": "3ea818d5-d2cd-4da9-a8a9-4173db369bf1"
      },
      "id": "OHUDfbHTNeL5",
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'alpha': 0,\n",
              " 'colsample_bylevel': 1,\n",
              " 'colsample_bynode': 1,\n",
              " 'colsample_bytree': 0.75,\n",
              " 'eta': 0.3,\n",
              " 'gamma': 0,\n",
              " 'lambda': 0,\n",
              " 'max_depth': 12,\n",
              " 'num_class': 20,\n",
              " 'num_parallel_tree': 25,\n",
              " 'objective': 'multi:softmax',\n",
              " 'predictor': 'gpu_predictor',\n",
              " 'sampling_method': 'gradient_based',\n",
              " 'subsample': 0.75,\n",
              " 'tree_method': 'gpu_hist'}"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open('/content/drive/MyDrive/Colab Notebooks/xgb_hyperparameters.json', mode='w') as file:\n",
        "  json.dump(best_params, file, indent=4)"
      ],
      "metadata": {
        "id": "_04IdznhM2HO"
      },
      "id": "_04IdznhM2HO",
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "xgb_model = xgb.train(\n",
        "    params                = best_params,\n",
        "    dtrain                = D_train,\n",
        "    evals                 = [(D_train,'train'), (D_test,'eval')],\n",
        "    num_boost_round       = 500,\n",
        "    early_stopping_rounds = 10\n",
        ")"
      ],
      "metadata": {
        "id": "wagznFy8nkfz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4f7ea113-faec-4a4d-b209-27e51b4c2baf"
      },
      "id": "wagznFy8nkfz",
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0]\ttrain-merror:0.059714\teval-merror:0.134952\n",
            "Multiple eval metrics have been passed: 'eval-merror' will be used for early stopping.\n",
            "\n",
            "Will train until eval-merror hasn't improved in 10 rounds.\n",
            "Stopping. Best iteration:\n",
            "[15]\ttrain-merror:0.000197\teval-merror:0.124074\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8759262178779758"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = xgb_model.predict(D_test, ntree_limit=xgb_model.best_ntree_limit).astype(np.int64)\n",
        "accuracy_score(y_test, y_pred)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CFP-WnrtCBRX",
        "outputId": "e470361f-b385-41cf-acb5-1361045eef9b"
      },
      "id": "CFP-WnrtCBRX",
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8759262178779758"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### CV"
      ],
      "metadata": {
        "id": "EZVYg3OhSFPp"
      },
      "id": "EZVYg3OhSFPp"
    },
    {
      "cell_type": "code",
      "source": [
        "D_train = xgb.DMatrix(X, label=y_encoder)\n",
        "\n",
        "df_xgb_cv = xgb.cv(\n",
        "    params                = best_params,\n",
        "    dtrain                = D_train,\n",
        "    num_boost_round       = 50,\n",
        "    early_stopping_rounds = 5,\n",
        "    nfold                 = 5,\n",
        "    stratified            = True,\n",
        "    verbose_eval          = True,\n",
        "    shuffle               = True\n",
        ")"
      ],
      "metadata": {
        "id": "SXOpf_PDCBJw"
      },
      "id": "SXOpf_PDCBJw",
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "1 - df_xgb_cv['test-merror-mean'].mean()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3A4E4jnkXs1K",
        "outputId": "7a9ef278-8b9d-4c60-9c32-571e2e7976ae"
      },
      "id": "3A4E4jnkXs1K",
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.883711256"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "5fd7cc4f",
        "BBvRsrAZK9Ej",
        "c8faec9d",
        "7a7e1440",
        "e4952b2a",
        "ff3a8d56",
        "be1736bf"
      ],
      "name": "Cópia de Cópia de final_project_part_i.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}